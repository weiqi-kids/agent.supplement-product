#!/usr/bin/env python3
"""
fetch_lnhpd_ingredients.py â€” ä¸‹è¼‰ Health Canada LNHPD æˆåˆ†è³‡æ–™

å¾ MedicinalIngredient API åˆ†é ä¸‹è¼‰æ‰€æœ‰æˆåˆ†è¨˜éŒ„ï¼ˆç´„ 810,000 ç­†ï¼‰ã€‚

ç”¨æ³•ï¼š
    python3 scripts/fetch_lnhpd_ingredients.py [--resume] [--limit N]

é¸é …ï¼š
    --resume    å¾ä¸Šæ¬¡ä¸­æ–·è™•ç¹¼çºŒä¸‹è¼‰ï¼ˆè®€å– .progress æª”æ¡ˆï¼‰
    --limit N   é™åˆ¶ä¸‹è¼‰ç­†æ•¸ï¼ˆæ¸¬è©¦ç”¨ï¼‰
    --output    æŒ‡å®šè¼¸å‡ºè·¯å¾‘ï¼ˆé è¨­ç‚º docs/Extractor/ca_lnhpd/raw/ingredients-YYYY-MM-DD.jsonlï¼‰

API ç«¯é»ï¼š
    https://health-products.canada.ca/api/natural-licences/medicinalingredient/?lang=en&type=json&page={N}&limit=100

æ³¨æ„ï¼š
    - API åˆ†é å¾ 1 é–‹å§‹
    - æ¯é æœ€å¤š 100 ç­†ï¼ˆAPI ç¡¬æ€§é™åˆ¶ï¼Œå³ä½¿è«‹æ±‚æ›´å¤šä¹Ÿåªå›å‚³ 100ï¼‰
    - ç´„éœ€ 5-6 å°æ™‚ä¸‹è¼‰å®Œæ•´è³‡æ–™é›†ï¼ˆ810K ç­† / 100 ç­†æ¯é  / ~40 ç­†æ¯ç§’ï¼‰
"""

import argparse
import json
import os
import sys
import time
from datetime import datetime
from urllib.request import urlopen, Request
from urllib.error import URLError, HTTPError

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
RAW_DIR = os.path.join(BASE_DIR, "docs/Extractor/ca_lnhpd/raw")

API_BASE = "https://health-products.canada.ca/api/natural-licences/medicinalingredient/"
PAGE_SIZE = 100  # API ç¡¬æ€§é™åˆ¶ï¼Œå³ä½¿è«‹æ±‚ 1000 ä¹Ÿåªæœƒå›å‚³ 100
MAX_RETRIES = 3
RETRY_DELAY_BASE = 5  # seconds, exponential backoff


def fetch_page(page_num: int, retries: int = MAX_RETRIES) -> dict | None:
    """
    ä¸‹è¼‰æŒ‡å®šé é¢çš„æˆåˆ†è³‡æ–™ã€‚

    Args:
        page_num: é ç¢¼ï¼ˆå¾ 1 é–‹å§‹ï¼‰
        retries: å‰©é¤˜é‡è©¦æ¬¡æ•¸

    Returns:
        API å›æ‡‰çš„ JSON ç‰©ä»¶ï¼Œå¤±æ•—æ™‚å›å‚³ None
    """
    url = f"{API_BASE}?lang=en&type=json&page={page_num}&limit={PAGE_SIZE}"

    req = Request(url)
    req.add_header("User-Agent", "SupplementProductIntelligence/1.0")
    req.add_header("Accept", "application/json")

    for attempt in range(retries):
        try:
            with urlopen(req, timeout=60) as response:
                data = json.loads(response.read().decode("utf-8"))
                return data
        except (URLError, HTTPError, json.JSONDecodeError) as e:
            delay = RETRY_DELAY_BASE * (2 ** attempt)
            print(f"  âš ï¸  ç¬¬ {page_num} é ä¸‹è¼‰å¤±æ•—ï¼ˆå˜—è©¦ {attempt + 1}/{retries}ï¼‰ï¼š{e}", file=sys.stderr)
            if attempt < retries - 1:
                print(f"     ç­‰å¾… {delay} ç§’å¾Œé‡è©¦...", file=sys.stderr)
                time.sleep(delay)
            else:
                print(f"  âŒ ç¬¬ {page_num} é ä¸‹è¼‰å¤±æ•—ï¼Œå·²é”é‡è©¦ä¸Šé™", file=sys.stderr)
                return None

    return None


def save_progress(progress_file: str, page_num: int, total_fetched: int):
    """å„²å­˜ä¸‹è¼‰é€²åº¦"""
    with open(progress_file, "w") as f:
        json.dump({
            "last_page": page_num,
            "total_fetched": total_fetched,
            "timestamp": datetime.now().isoformat()
        }, f)


def load_progress(progress_file: str) -> tuple[int, int]:
    """è¼‰å…¥ä¸‹è¼‰é€²åº¦ï¼Œå›å‚³ (last_page, total_fetched)"""
    if os.path.exists(progress_file):
        try:
            with open(progress_file, "r") as f:
                data = json.load(f)
                return data.get("last_page", 0), data.get("total_fetched", 0)
        except (json.JSONDecodeError, KeyError):
            pass
    return 0, 0


def main():
    parser = argparse.ArgumentParser(
        description="ä¸‹è¼‰ Health Canada LNHPD æˆåˆ†è³‡æ–™"
    )
    parser.add_argument(
        "--resume", "-r",
        action="store_true",
        help="å¾ä¸Šæ¬¡ä¸­æ–·è™•ç¹¼çºŒä¸‹è¼‰"
    )
    parser.add_argument(
        "--limit", "-l",
        type=int,
        default=0,
        help="é™åˆ¶ä¸‹è¼‰ç­†æ•¸ï¼ˆæ¸¬è©¦ç”¨ï¼‰"
    )
    parser.add_argument(
        "--output", "-o",
        type=str,
        default="",
        help="æŒ‡å®šè¼¸å‡ºè·¯å¾‘"
    )
    args = parser.parse_args()

    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    os.makedirs(RAW_DIR, exist_ok=True)

    # è¨­å®šè¼¸å‡ºæª”æ¡ˆ
    today = datetime.now().strftime("%Y-%m-%d")
    if args.output:
        output_file = args.output
    else:
        output_file = os.path.join(RAW_DIR, f"ingredients-{today}.jsonl")

    progress_file = output_file + ".progress"
    latest_link = os.path.join(RAW_DIR, "latest-ingredients.jsonl")

    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("ğŸ“¡ LNHPD æˆåˆ†è³‡æ–™ä¸‹è¼‰")
    print(f"   API: {API_BASE}")
    print(f"   è¼¸å‡º: {output_file}")
    if args.limit > 0:
        print(f"   é™åˆ¶: {args.limit} ç­†")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print()

    # è™•ç†æ–·é»çºŒå‚³
    start_page = 1
    total_fetched = 0
    write_mode = "w"

    if args.resume:
        last_page, prev_fetched = load_progress(progress_file)
        if last_page > 0:
            start_page = last_page + 1
            total_fetched = prev_fetched
            write_mode = "a"
            print(f"ğŸ“¥ å¾ç¬¬ {start_page} é ç¹¼çºŒä¸‹è¼‰ï¼ˆå·²æœ‰ {total_fetched} ç­†ï¼‰")
        else:
            print("â„¹ï¸  ç„¡é€²åº¦æª”æ¡ˆï¼Œå¾é ­é–‹å§‹ä¸‹è¼‰")

    # é–‹å§‹ä¸‹è¼‰
    page_num = start_page
    empty_pages = 0
    start_time = time.time()

    with open(output_file, write_mode, encoding="utf-8") as f:
        while True:
            # é™åˆ¶ç­†æ•¸æª¢æŸ¥
            if args.limit > 0 and total_fetched >= args.limit:
                print(f"\nâœ… å·²é”é™åˆ¶ç­†æ•¸ {args.limit}ï¼Œåœæ­¢ä¸‹è¼‰")
                break

            # ä¸‹è¼‰é é¢
            data = fetch_page(page_num)

            if data is None:
                print(f"\nâŒ ç¬¬ {page_num} é ä¸‹è¼‰å¤±æ•—ï¼Œä¸­æ–·ä¸‹è¼‰")
                print(f"   å·²ä¸‹è¼‰ {total_fetched} ç­†ï¼Œé€²åº¦å·²å„²å­˜")
                save_progress(progress_file, page_num - 1, total_fetched)
                sys.exit(1)

            # æª¢æŸ¥æ˜¯å¦ç‚ºç©ºé 
            records = data if isinstance(data, list) else data.get("data", [])

            if not records:
                empty_pages += 1
                if empty_pages >= 3:
                    # é€£çºŒä¸‰å€‹ç©ºé ï¼Œè¦–ç‚ºå·²åˆ°é”çµå°¾
                    print(f"\nâœ… é€£çºŒ {empty_pages} å€‹ç©ºé ï¼Œä¸‹è¼‰å®Œæˆ")
                    break
                page_num += 1
                continue

            empty_pages = 0  # é‡ç½®ç©ºé è¨ˆæ•¸

            # å¯«å…¥ JSONL
            page_count = 0
            for record in records:
                if args.limit > 0 and total_fetched >= args.limit:
                    break
                f.write(json.dumps(record, ensure_ascii=False) + "\n")
                total_fetched += 1
                page_count += 1

            # é€²åº¦å›å ±
            elapsed = time.time() - start_time
            rate = total_fetched / elapsed if elapsed > 0 else 0
            print(f"âœ… ç¬¬ {page_num:4d} é ï¼š{page_count:4d} ç­† | ç´¯è¨ˆ {total_fetched:,} ç­† | {rate:.0f} ç­†/ç§’")

            # å„²å­˜é€²åº¦ï¼ˆæ¯ 10 é ï¼‰
            if page_num % 10 == 0:
                save_progress(progress_file, page_num, total_fetched)

            page_num += 1

            # çŸ­æš«å»¶é²ï¼Œé¿å…å° API é€ æˆå£“åŠ›
            time.sleep(0.1)

    # æ¸…ç†é€²åº¦æª”æ¡ˆ
    if os.path.exists(progress_file):
        os.remove(progress_file)

    # æ›´æ–°ç¬¦è™Ÿé€£çµ
    if os.path.islink(latest_link):
        os.unlink(latest_link)
    elif os.path.exists(latest_link):
        os.remove(latest_link)
    os.symlink(os.path.basename(output_file), latest_link)

    # æœ€çµ‚çµ±è¨ˆ
    elapsed = time.time() - start_time
    print()
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("ğŸ“Š ä¸‹è¼‰å®Œæˆçµ±è¨ˆ")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print(f"   ç¸½ç­†æ•¸ï¼š{total_fetched:,}")
    print(f"   ç¸½é æ•¸ï¼š{page_num - start_page + 1}")
    print(f"   è€—æ™‚ï¼š{elapsed:.1f} ç§’ï¼ˆ{elapsed/60:.1f} åˆ†é˜ï¼‰")
    print(f"   è¼¸å‡ºï¼š{output_file}")
    print(f"   é€£çµï¼šlatest-ingredients.jsonl â†’ {os.path.basename(output_file)}")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")


if __name__ == "__main__":
    main()
