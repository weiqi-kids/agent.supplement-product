#!/usr/bin/env python3
"""
ä¸»é¡Œå ±å‘Šç”¢å‡ºè…³æœ¬

åŠŸèƒ½ï¼š
1. è®€å– topics/*.yaml å–å¾—è¿½è¹¤ä¸»é¡Œå®šç¾©
2. å¾ docs/Extractor/{layer}/{category}/*.md ç¯©é¸ç¬¦åˆé—œéµè©çš„ç”¢å“
3. ç”¢å‡ºæœˆåº¦å¸‚å ´å ±å‘Š

ç”¨æ³•ï¼š
  python3 scripts/generate_topic_report.py                    # ç”¢å‡ºæ‰€æœ‰ä¸»é¡Œå ±å‘Š
  python3 scripts/generate_topic_report.py --topic exosomes   # ç”¢å‡ºç‰¹å®šä¸»é¡Œ
  python3 scripts/generate_topic_report.py --dry-run          # Dry run æ¨¡å¼
"""

import argparse
import re
import yaml
from pathlib import Path
from datetime import datetime
from collections import defaultdict
from typing import Optional


# è·¯å¾‘é…ç½®
PROJECT_ROOT = Path(__file__).parent.parent
TOPICS_DIR = PROJECT_ROOT / "core" / "Narrator" / "Modes" / "topic_tracking" / "topics"
EXTRACTOR_DIR = PROJECT_ROOT / "docs" / "Extractor"
OUTPUT_DIR = PROJECT_ROOT / "docs" / "Narrator" / "topic_tracking"
DHI_DIR = EXTRACTOR_DIR / "dhi"

# ä¸»é¡Œèˆ‡ DHI é¡åˆ¥å°ç…§
TOPIC_DHI_CATEGORIES = {
    "fish-oil": ["omega_fatty_acid"],
    "curcumin": ["botanical"],
    "nattokinase": ["enzyme"],
    "glucosamine": ["amino_acid"],
    "collagen": ["protein"],
    "lutein": ["vitamin"],
    "nmn": ["other"],
    "red-yeast-rice": ["botanical"],
    "exosomes": ["other"],
}

# Layer å°æ‡‰å¸‚å ´
LAYER_MARKET = {
    "us_dsld": {"name": "ç¾åœ‹", "flag": "ğŸ‡ºğŸ‡¸", "code": "US"},
    "ca_lnhpd": {"name": "åŠ æ‹¿å¤§", "flag": "ğŸ‡¨ğŸ‡¦", "code": "CA"},
    "kr_hff": {"name": "éŸ“åœ‹", "flag": "ğŸ‡°ğŸ‡·", "code": "KR"},
    "jp_fnfc": {"name": "æ—¥æœ¬ FNFC", "flag": "ğŸ‡¯ğŸ‡µ", "code": "JP"},
    "jp_foshu": {"name": "æ—¥æœ¬ FOSHU", "flag": "ğŸ‡¯ğŸ‡µ", "code": "JP"},
    "tw_hf": {"name": "å°ç£", "flag": "ğŸ‡¹ğŸ‡¼", "code": "TW"},
}


def load_topic(topic_path: Path) -> dict:
    """è¼‰å…¥ä¸»é¡Œå®šç¾©"""
    with open(topic_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def load_all_topics() -> list[dict]:
    """è¼‰å…¥æ‰€æœ‰ä¸»é¡Œå®šç¾©"""
    topics = []
    for yaml_file in TOPICS_DIR.glob("*.yaml"):
        topic = load_topic(yaml_file)
        topic["_file"] = yaml_file.name
        topics.append(topic)
    return topics


def parse_product_file(file_path: Path) -> dict:
    """è§£æç”¢å“ Markdown æª”æ¡ˆ"""
    content = file_path.read_text(encoding="utf-8")

    # æª¢æŸ¥ REVIEW_NEEDED æ¨™è¨˜
    if "[REVIEW_NEEDED]" in content:
        return None

    product = {
        "file_path": str(file_path),
        "content": content,
        "name": "",
        "brand": "",
        "manufacturer": "",
        "ingredients": [],
        "form": "",
        "layer": file_path.parent.parent.name,
        "category": file_path.parent.name,
    }

    # è§£æ frontmatter
    if content.startswith("---"):
        parts = content.split("---", 2)
        if len(parts) >= 3:
            try:
                fm = yaml.safe_load(parts[1])
                if fm:
                    product["name"] = fm.get("product_name", "")
                    product["brand"] = fm.get("brand", "")
                    product["manufacturer"] = fm.get("manufacturer", "")
                    product["form"] = fm.get("product_form", "")
            except yaml.YAMLError:
                pass

    # æå–æˆåˆ†ï¼ˆå¾ ## æˆåˆ† æˆ– ## æ©Ÿèƒ½æ€§æˆåˆ† æ®µè½ï¼‰
    ingredient_patterns = [
        r"## æˆåˆ†\s*\n([\s\S]*?)(?=\n##|\Z)",
        r"## æ©Ÿèƒ½æ€§æˆåˆ†\s*\n([\s\S]*?)(?=\n##|\Z)",
        r"## æ©Ÿèƒ½æ€§é–¢ä¸æˆåˆ†\s*\n([\s\S]*?)(?=\n##|\Z)",
        r"## Ingredients\s*\n([\s\S]*?)(?=\n##|\Z)",
    ]

    for pattern in ingredient_patterns:
        match = re.search(pattern, content, re.IGNORECASE)
        if match:
            ingredients_text = match.group(1)
            # æå–åˆ—è¡¨é …ç›®
            for line in ingredients_text.split("\n"):
                line = line.strip()
                if line.startswith("- "):
                    product["ingredients"].append(line[2:].strip())
                elif line and not line.startswith("#"):
                    product["ingredients"].append(line)

    return product


def match_product(product: dict, topic: dict) -> bool:
    """æª¢æŸ¥ç”¢å“æ˜¯å¦åŒ¹é…ä¸»é¡Œé—œéµè©"""
    if not product:
        return False

    keywords = topic.get("keywords", {})
    exact_keywords = [k.lower() for k in keywords.get("exact", [])]
    fuzzy_keywords = [k.lower() for k in keywords.get("fuzzy", [])]

    # ç²¾ç¢ºåŒ¹é…æˆåˆ†
    for ingredient in product.get("ingredients", []):
        ingredient_lower = ingredient.lower()
        for keyword in exact_keywords:
            if keyword in ingredient_lower:
                return True

    # æ¨¡ç³ŠåŒ¹é…ç”¢å“åç¨±å’Œå…§å®¹
    search_text = " ".join([
        product.get("name", ""),
        product.get("content", ""),
    ]).lower()

    for keyword in exact_keywords + fuzzy_keywords:
        if keyword in search_text:
            return True

    return False


def load_interaction_data(topic_id: str) -> list[dict]:
    """
    å¾ docs/Extractor/dhi/{category}/*.md è®€å–äº¤äº’è³‡æ–™
    æ ¹æ“šä¸»é¡Œ ID ç¯©é¸ç›¸é—œé¡åˆ¥
    """
    interactions = []
    categories = TOPIC_DHI_CATEGORIES.get(topic_id, [])

    if not categories:
        return interactions

    for category in categories:
        category_dir = DHI_DIR / category
        if not category_dir.exists():
            continue

        for md_file in category_dir.glob("*.md"):
            content = md_file.read_text(encoding="utf-8")

            # è·³é REVIEW_NEEDEDï¼ˆä½†ä»è§£æä»¥ç²å–è³‡è¨Šï¼‰
            is_review_needed = "[REVIEW_NEEDED]" in content

            # è§£æ frontmatter
            interaction = {
                "file_path": str(md_file),
                "pmid": md_file.stem,
                "is_review_needed": is_review_needed,
            }

            if content.startswith("---") or content.startswith("[REVIEW_NEEDED]\n\n---"):
                # è™•ç† [REVIEW_NEEDED] æ¨™è¨˜å¾Œçš„ frontmatter
                clean_content = content.replace("[REVIEW_NEEDED]\n\n", "")
                parts = clean_content.split("---", 2)
                if len(parts) >= 3:
                    try:
                        fm = yaml.safe_load(parts[1])
                        if fm:
                            interaction["title"] = fm.get("title", "")
                            interaction["severity"] = fm.get("severity", "unknown")
                            interaction["evidence_level"] = fm.get("evidence_level", 5)
                            interaction["study_type"] = fm.get("study_type", "other")
                            interaction["source_url"] = fm.get("source_url", "")
                            interaction["journal"] = fm.get("journal", "")
                            interaction["pub_date"] = fm.get("pub_date", "")
                    except yaml.YAMLError:
                        pass

            # æå–æ‘˜è¦ï¼ˆç”¨æ–¼æ¨æ–·é¢¨éšªæè¿°ï¼‰
            abstract_match = re.search(r"## æ‘˜è¦\s*\n([\s\S]*?)(?=\n---|\Z)", content)
            if abstract_match:
                interaction["abstract"] = abstract_match.group(1).strip()
            else:
                interaction["abstract"] = ""

            interactions.append(interaction)

    return interactions


def infer_risk_description(interaction: dict) -> str:
    """å¾æ‘˜è¦æ¨æ–·é¢¨éšªæè¿°"""
    abstract = interaction.get("abstract", "").lower()
    title = interaction.get("title", "").lower()
    text = f"{title} {abstract}"

    # å¸¸è¦‹é¢¨éšªé—œéµè©
    if any(kw in text for kw in ["bleeding", "hemorrhag", "coagulopathy"]):
        return "å¢åŠ å‡ºè¡€é¢¨éšª"
    if any(kw in text for kw in ["hypoglycemia", "blood glucose", "blood sugar"]):
        return "å½±éŸ¿è¡€ç³–æ§åˆ¶"
    if any(kw in text for kw in ["hypotension", "blood pressure"]):
        return "å½±éŸ¿è¡€å£“"
    if any(kw in text for kw in ["hepatotoxic", "liver"]):
        return "è‚è‡Ÿä»£è¬å½±éŸ¿"
    if any(kw in text for kw in ["no effect", "no significant", "did not affect"]):
        return "ç„¡é¡¯è‘—äº¤äº’"
    return "äº¤äº’æ©Ÿè½‰å¾…é‡æ¸…"


def format_interaction_section(topic_name: str, interactions: list[dict]) -> str:
    """ç”¢ç”Ÿäº¤äº’ä½œç”¨ç« ç¯€ Markdown"""
    if not interactions:
        return ""

    # æŒ‰åš´é‡ç¨‹åº¦åˆ†çµ„
    by_severity = defaultdict(list)
    for interaction in interactions:
        severity = interaction.get("severity", "unknown")
        by_severity[severity].append(interaction)

    section = f"""
## âš ï¸ è—¥ç‰©äº¤äº’æé†’

æœ¬ç« ç¯€æ•´ç†{topic_name}ç›¸é—œçš„è—¥ç‰©äº¤äº’æ–‡ç»ï¼Œä¾›åƒè€ƒã€‚

"""

    severity_order = ["major", "moderate", "minor", "unknown"]
    severity_labels = {
        "major": "é«˜é¢¨éšªäº¤äº’ (Major)",
        "moderate": "ä¸­ç­‰é¢¨éšªäº¤äº’ (Moderate)",
        "minor": "ä½é¢¨éšª/ç„¡äº¤äº’ (Minor)",
        "unknown": "å¾…è©•ä¼° (Unknown)",
    }

    has_content = False
    for severity in severity_order:
        items = by_severity.get(severity, [])
        if not items:
            continue

        has_content = True
        section += f"### {severity_labels[severity]}\n\n"
        section += "| æ–‡ç»æ¨™é¡Œ | é¢¨éšªæè¿° | è­‰æ“šç­‰ç´š | ä¾†æº |\n"
        section += "|----------|----------|----------|------|\n"

        for item in items:
            title = item.get("title", "")[:50]
            if len(item.get("title", "")) > 50:
                title += "..."
            risk_desc = infer_risk_description(item)
            level = item.get("evidence_level", 5)
            pmid = item.get("pmid", "")
            url = item.get("source_url", f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/")
            section += f"| {title} | {risk_desc} | Level {level} | [PMID:{pmid}]({url}) |\n"

        section += "\n"

    if not has_content:
        return ""

    section += """> âš ï¸ **å…è²¬è²æ˜**ï¼šæœ¬è³‡è¨Šåƒ…ä¾›æ•™è‚²å’Œç ”ç©¶ç›®çš„ï¼Œä¸æ§‹æˆé†«ç™‚å»ºè­°ã€‚
> ä»»ä½•ç”¨è—¥æˆ–è£œå……åŠ‘è®Šæ›´æ‡‰è«®è©¢å°ˆæ¥­é†«ç™‚äººå“¡ã€‚

"""

    return section


def scan_products(topic: dict, dry_run: bool = False) -> list[dict]:
    """æƒæä¸¦ç¯©é¸ç¬¦åˆä¸»é¡Œçš„ç”¢å“"""
    matched_products = []
    category_filter = topic.get("category_filter", [])

    for layer_dir in EXTRACTOR_DIR.iterdir():
        if not layer_dir.is_dir():
            continue
        if layer_dir.name not in LAYER_MARKET:
            continue

        # éæ­·åˆ†é¡ç›®éŒ„
        for category_dir in layer_dir.iterdir():
            if not category_dir.is_dir():
                continue
            if category_dir.name == "raw":
                continue

            # å¦‚æœæœ‰åˆ†é¡éæ¿¾ï¼Œæª¢æŸ¥æ˜¯å¦ç¬¦åˆ
            if category_filter and category_dir.name not in category_filter:
                continue

            # æƒæç”¢å“æª”æ¡ˆ
            for product_file in category_dir.glob("*.md"):
                product = parse_product_file(product_file)
                if product and match_product(product, topic):
                    matched_products.append(product)

    return matched_products


def generate_report(topic: dict, products: list[dict]) -> str:
    """ç”¢ç”Ÿå¸‚å ´å ±å‘Š"""
    topic_id = topic["topic_id"]
    topic_name = topic["name"]["zh"]
    now = datetime.now()
    period = now.strftime("%Y-%m")

    # çµ±è¨ˆæ•¸æ“š
    by_layer = defaultdict(list)
    by_brand = defaultdict(int)
    by_form = defaultdict(int)

    for product in products:
        layer = product["layer"]
        by_layer[layer].append(product)

        brand = product.get("brand") or product.get("manufacturer") or "æœªçŸ¥"
        by_brand[brand] += 1

        form = product.get("form") or "æœªçŸ¥"
        by_form[form] += 1

    # ç”¢ç”Ÿå ±å‘Š
    report = f"""---
topic: {topic_id}
period: "{period}"
generated_at: "{now.isoformat()}"
source_layers:
"""
    for layer in LAYER_MARKET.keys():
        if layer in by_layer:
            report += f"  - {layer}\n"

    report += f"""---

# {topic_name}å¸‚å ´å ±å‘Š â€” {now.year} å¹´ {now.month} æœˆ

> å ±å‘ŠæœŸé–“ï¼š{now.strftime("%Y-%m")}-01 ~ {now.strftime("%Y-%m-%d")}
> ç”¢å‡ºæ™‚é–“ï¼š{now.isoformat()}

## æ‘˜è¦

æœ¬æœˆ{topic_name}ä¸»é¡Œè¿½è¹¤å…±ç¯©é¸å‡º {len(products)} ç­†ç›¸é—œç”¢å“ï¼Œæ¶µè“‹ {len(by_layer)} å€‹å¸‚å ´ã€‚
"""

    # å¸‚å ´æ¦‚æ³
    if by_layer:
        top_market = max(by_layer.items(), key=lambda x: len(x[1]))
        market_info = LAYER_MARKET.get(top_market[0], {})
        report += f"{market_info.get('name', top_market[0])} å¸‚å ´ä»¥ {len(top_market[1])} ç­†ç”¢å“å±…é¦–ã€‚"

    report += """

## å„åœ‹ç”¢å“çµ±è¨ˆ

| å¸‚å ´ | ç”¢å“æ•¸ | ä¸»è¦å“ç‰Œ |
|------|--------|----------|
"""

    for layer, layer_products in sorted(by_layer.items(), key=lambda x: -len(x[1])):
        market_info = LAYER_MARKET.get(layer, {"flag": "", "name": layer})

        # çµ±è¨ˆè©²å¸‚å ´çš„å“ç‰Œ
        layer_brands = defaultdict(int)
        for p in layer_products:
            brand = p.get("brand") or p.get("manufacturer") or "æœªçŸ¥"
            layer_brands[brand] += 1
        top_brands = sorted(layer_brands.items(), key=lambda x: -x[1])[:3]
        brands_str = ", ".join([b[0] for b in top_brands if b[0] != "æœªçŸ¥"][:2]) or "-"

        report += f"| {market_info['flag']} {market_info['name']} | {len(layer_products)} | {brands_str} |\n"

    # ç†±é–€å“ç‰Œ
    report += """
## ç†±é–€å“ç‰Œ/è£½é€ å•†

| æ’å | å“ç‰Œ/è£½é€ å•† | ç”¢å“æ•¸ |
|------|-------------|--------|
"""

    top_brands = sorted(by_brand.items(), key=lambda x: -x[1])[:10]
    for i, (brand, count) in enumerate(top_brands, 1):
        if brand != "æœªçŸ¥":
            report += f"| {i} | {brand} | {count} |\n"

    # åŠ‘å‹åˆ†å¸ƒ
    report += """
## åŠ‘å‹åˆ†å¸ƒ

| åŠ‘å‹ | ç”¢å“æ•¸ | ä½”æ¯” |
|------|--------|------|
"""

    total = len(products)
    for form, count in sorted(by_form.items(), key=lambda x: -x[1]):
        pct = count / total * 100 if total > 0 else 0
        report += f"| {form} | {count} | {pct:.1f}% |\n"

    # è—¥ç‰©äº¤äº’ç« ç¯€
    interactions = load_interaction_data(topic_id)
    if interactions:
        interaction_section = format_interaction_section(topic_name, interactions)
        report += interaction_section

    # è¶¨å‹¢è§€å¯Ÿ
    report += f"""
## è¶¨å‹¢è§€å¯Ÿ

æœ¬æœˆ{topic_name}ç›¸é—œç”¢å“ä¸»è¦é›†ä¸­åœ¨ä»¥ä¸‹å¸‚å ´èˆ‡é¡å‹ï¼š

"""

    if by_layer:
        for layer in list(by_layer.keys())[:3]:
            market_info = LAYER_MARKET.get(layer, {"name": layer})
            report += f"- **{market_info['name']}**ï¼š{len(by_layer[layer])} ç­†ç”¢å“\n"

    report += """
---

*æœ¬å ±å‘Šç”±ç³»çµ±è‡ªå‹•ç”¢ç”Ÿï¼Œè³‡æ–™ä¾†æºç‚ºå„åœ‹å®˜æ–¹ä¿å¥é£Ÿå“è³‡æ–™åº«ã€‚*
"""

    return report


def main():
    parser = argparse.ArgumentParser(description="ä¸»é¡Œå ±å‘Šç”¢å‡ºè…³æœ¬")
    parser.add_argument("--topic", help="æŒ‡å®šä¸»é¡Œ IDï¼ˆä¸æŒ‡å®šå‰‡è™•ç†æ‰€æœ‰ä¸»é¡Œï¼‰")
    parser.add_argument("--dry-run", action="store_true", help="Dry run æ¨¡å¼ï¼Œåƒ…é¡¯ç¤ºåŒ¹é…çµæœ")
    args = parser.parse_args()

    print("=" * 50)
    print("ä¸»é¡Œå ±å‘Šç”¢å‡º")
    print("=" * 50)

    # è¼‰å…¥ä¸»é¡Œ
    topics = load_all_topics()
    if args.topic:
        topics = [t for t in topics if t["topic_id"] == args.topic]
        if not topics:
            print(f"âŒ æ‰¾ä¸åˆ°ä¸»é¡Œ: {args.topic}")
            return

    print(f"ğŸ“‹ è¼‰å…¥ {len(topics)} å€‹ä¸»é¡Œå®šç¾©")

    for topic in topics:
        topic_id = topic["topic_id"]
        topic_name = topic["name"]["zh"]

        print(f"\n{'='*50}")
        print(f"ğŸ“Š è™•ç†ä¸»é¡Œ: {topic_name} ({topic_id})")
        print(f"{'='*50}")

        # æƒæç”¢å“
        products = scan_products(topic, args.dry_run)
        print(f"âœ… åŒ¹é…ç”¢å“: {len(products)} ç­†")

        if args.dry_run:
            # é¡¯ç¤ºåŒ¹é…çµæœ
            by_layer = defaultdict(int)
            for p in products:
                by_layer[p["layer"]] += 1

            print("\nå„å¸‚å ´åŒ¹é…æ•¸é‡:")
            for layer, count in sorted(by_layer.items(), key=lambda x: -x[1]):
                market_info = LAYER_MARKET.get(layer, {"flag": "", "name": layer})
                print(f"  {market_info['flag']} {market_info['name']}: {count}")

            print("\nç¯„ä¾‹ç”¢å“ (å‰ 5 ç­†):")
            for p in products[:5]:
                print(f"  - [{p['layer']}] {p['name'][:50]}...")
        else:
            # ç”¢ç”Ÿå ±å‘Š
            report = generate_report(topic, products)

            # å¯«å…¥æª”æ¡ˆ
            output_dir = OUTPUT_DIR / topic_id
            output_dir.mkdir(parents=True, exist_ok=True)

            period = datetime.now().strftime("%Y-%m")
            output_file = output_dir / f"{period}.md"
            output_file.write_text(report, encoding="utf-8")

            print(f"ğŸ“ å ±å‘Šå·²å¯«å…¥: {output_file.relative_to(PROJECT_ROOT)}")

    print("\n" + "=" * 50)
    print("âœ… å®Œæˆ")
    print("=" * 50)


if __name__ == "__main__":
    main()
