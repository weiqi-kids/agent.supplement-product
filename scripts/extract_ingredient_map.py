#!/usr/bin/env python3
"""æˆåˆ†æ¨™æº–åŒ–èƒå–è…³æœ¬ â€” å°‡ JSONL è½‰æ›ç‚º .md æª”"""
import json
import os
import re
import sys
import argparse
from datetime import datetime
from pathlib import Path

BASE_DIR = Path(__file__).parent.parent
RAW_DIR = BASE_DIR / "docs" / "Extractor" / "ingredient_map" / "raw"
OUTPUT_DIR = BASE_DIR / "docs" / "Extractor" / "ingredient_map"

# æˆåˆ†åˆ†é¡é—œéµè©
INGREDIENT_CATEGORY_KEYWORDS = {
    "vitamin": ["vitamin", "ascorbic", "tocopherol", "retinol", "folate", "biotin",
                "niacin", "riboflavin", "thiamin", "cobalamin", "pyridoxine"],
    "mineral": ["calcium", "magnesium", "zinc", "iron", "selenium", "chromium",
                "copper", "manganese", "potassium", "iodine", "sodium"],
    "fatty_acid": ["omega", "epa", "dha", "fatty acid", "fish oil", "krill",
                   "flaxseed", "ala", "linoleic"],
    "amino_acid": ["amino", "glutamine", "arginine", "lysine", "bcaa", "leucine",
                   "carnitine", "tryptophan", "tyrosine", "glycine"],
    "protein": ["protein", "collagen", "whey", "casein", "albumin"],
    "botanical": ["extract", "herb", "plant", "root", "leaf", "flower", "berry",
                  "ginseng", "ginkgo", "turmeric", "curcumin", "green tea"],
    "probiotic": ["probiotic", "lactobacillus", "bifidobacterium", "acidophilus",
                  "rhamnosus", "saccharomyces"],
    "enzyme": ["enzyme", "nattokinase", "bromelain", "papain", "lipase", "protease"],
    "hormone": ["dhea", "melatonin", "pregnenolone"],
}


def slugify(text: str) -> str:
    """è½‰æ›ç‚º URL-safe çš„ slug"""
    text = text.lower()
    text = re.sub(r"[^\w\s-]", "", text)
    text = re.sub(r"[-\s]+", "-", text)
    return text.strip("-")


def categorize_ingredient(name: str) -> str:
    """åˆ†é¡æˆåˆ†"""
    name_lower = name.lower()

    for category, keywords in INGREDIENT_CATEGORY_KEYWORDS.items():
        for kw in keywords:
            if kw in name_lower:
                return category

    return "other"


def check_review_needed(item: dict) -> list:
    """æª¢æŸ¥æ˜¯å¦éœ€è¦æ¨™è¨˜ REVIEW_NEEDED"""
    reasons = []

    if not item.get("rxnorm_id") and item.get("confidence") == "low":
        reasons.append("ç„¡ RxNorm ID ä¸”ä¿¡å¿ƒåº¦ä½")

    if item.get("match_type") == "approximate" and item.get("confidence") == "low":
        reasons.append("æ¨¡ç³ŠåŒ¹é…ä¸”ä¿¡å¿ƒåº¦ä½")

    return reasons


def generate_markdown(item: dict) -> str:
    """ç”Ÿæˆ Markdown å…§å®¹"""
    term = item.get("term", "")
    standard_name = item.get("standard_name") or term
    rxnorm_id = item.get("rxnorm_id", "")
    confidence = item.get("confidence", "low")
    match_type = item.get("match_type", "")
    frequency = item.get("frequency", 0)

    slug = slugify(term)
    category = categorize_ingredient(standard_name)

    review_reasons = check_review_needed(item)
    review_marker = "[REVIEW_NEEDED]\n\n" if review_reasons else ""

    source_url = f"https://rxnav.nlm.nih.gov/REST/rxcui/{rxnorm_id}" if rxnorm_id else ""

    md = f"""{review_marker}---
source_id: "{slug}"
source_layer: "ingredient_map"
source_url: "{source_url}"
standard_name: "{standard_name}"
standard_name_zh: ""
rxnorm_id: "{rxnorm_id or ''}"
original_term: "{term}"
category: "{category}"
confidence: "{confidence}"
match_type: "{match_type or ''}"
frequency: {frequency}
fetched_at: "{item.get('queried_at', datetime.now().isoformat())}"
---

# {standard_name}

## åŸºæœ¬è³‡è¨Š
- æ¨™æº–åç¨±ï¼š{standard_name}
- åŸå§‹æŸ¥è©¢ï¼š{term}
- RxNorm IDï¼š{rxnorm_id or 'N/A'}
- åˆ†é¡ï¼š{category}
- åŒ¹é…é¡å‹ï¼š{match_type or 'N/A'}
- åŒ¹é…ä¿¡å¿ƒåº¦ï¼š{confidence}
- å‡ºç¾é »ç‡ï¼š{frequency} æ¬¡

## åˆ¥å
- {term}
"""

    if review_reasons:
        md += f"""
## éœ€è¦å¯©æ ¸
- {chr(10).join('- ' + r for r in review_reasons)}
"""

    return md


def get_existing_source_ids() -> dict:
    """å–å¾—å·²å­˜åœ¨çš„ source_id åŠå…¶è·¯å¾‘"""
    existing = {}

    for category_dir in OUTPUT_DIR.iterdir():
        if not category_dir.is_dir() or category_dir.name == "raw":
            continue

        for md_file in category_dir.glob("*.md"):
            try:
                content = md_file.read_text(encoding="utf-8")
                for line in content.split("\n"):
                    if line.startswith("source_id:"):
                        sid = line.split(":", 1)[1].strip().strip('"')
                        existing[sid] = md_file
                        break
            except Exception:
                pass

    return existing


def process_jsonl_file(jsonl_path: Path, force: bool = False) -> dict:
    """è™•ç†å–®ä¸€ JSONL æª”æ¡ˆ"""
    stats = {"processed": 0, "skipped": 0, "review_needed": 0, "new": 0}

    existing = {} if force else get_existing_source_ids()

    with open(jsonl_path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue

            try:
                item = json.loads(line)
            except json.JSONDecodeError:
                continue

            term = item.get("term", "")
            if not term:
                stats["skipped"] += 1
                continue

            slug = slugify(term)

            # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if slug in existing and not force:
                stats["skipped"] += 1
                continue

            # åˆ†é¡
            standard_name = item.get("standard_name") or term
            category = categorize_ingredient(standard_name)

            # å»ºç«‹ç›®éŒ„
            category_dir = OUTPUT_DIR / category
            category_dir.mkdir(parents=True, exist_ok=True)

            # ç”Ÿæˆ Markdown
            md_content = generate_markdown(item)

            # å¯«å…¥æª”æ¡ˆ
            output_path = category_dir / f"{slug}.md"
            output_path.write_text(md_content, encoding="utf-8")

            stats["processed"] += 1
            stats["new"] += 1

            if check_review_needed(item):
                stats["review_needed"] += 1

    return stats


def main():
    parser = argparse.ArgumentParser(description="æˆåˆ†æ¨™æº–åŒ–èƒå–")
    parser.add_argument("jsonl_file", nargs="?", help="æŒ‡å®š JSONL æª”æ¡ˆ")
    parser.add_argument("--all", action="store_true", help="è™•ç†æ‰€æœ‰ JSONL æª”æ¡ˆ")
    parser.add_argument("--force", action="store_true", help="å¼·åˆ¶è¦†è“‹å·²å­˜åœ¨çš„æª”æ¡ˆ")
    args = parser.parse_args()

    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("ğŸ§ª æˆåˆ†æ¨™æº–åŒ–èƒå–")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

    # æ‰¾åˆ°è¦è™•ç†çš„æª”æ¡ˆ
    if args.jsonl_file:
        jsonl_files = [Path(args.jsonl_file)]
    elif args.all:
        jsonl_files = list(RAW_DIR.glob("normalized_*.jsonl"))
    else:
        # æ‰¾æœ€æ–°çš„
        jsonl_files = sorted(RAW_DIR.glob("normalized_*.jsonl"),
                            key=lambda x: x.stat().st_mtime,
                            reverse=True)[:1]

    if not jsonl_files:
        print("  æ‰¾ä¸åˆ° JSONL æª”æ¡ˆï¼Œè«‹å…ˆåŸ·è¡Œ fetch.sh", file=sys.stderr)
        sys.exit(1)

    total_stats = {"processed": 0, "skipped": 0, "review_needed": 0, "new": 0}

    for jsonl_path in jsonl_files:
        print(f"\n  è™•ç†: {jsonl_path.name}")
        stats = process_jsonl_file(jsonl_path, args.force)

        for k, v in stats.items():
            total_stats[k] += v

        print(f"    âœ… è™•ç†: {stats['processed']}, è·³é: {stats['skipped']}, REVIEW: {stats['review_needed']}")

    print("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print(f"âœ… èƒå–å®Œæˆ")
    print(f"   è™•ç†: {total_stats['processed']}")
    print(f"   è·³é: {total_stats['skipped']}")
    print(f"   REVIEW_NEEDED: {total_stats['review_needed']}")


if __name__ == "__main__":
    main()
