#!/usr/bin/env python3
"""æˆåˆ†æ¨™æº–åŒ–æ“·å–è…³æœ¬ â€” å¾ç”¢å“èƒå–æˆåˆ†ä¸¦é€é RxNorm API æ¨™æº–åŒ–"""
import json
import os
import re
import sys
import argparse
import time
from datetime import datetime
from collections import Counter
from pathlib import Path

try:
    import requests
except ImportError:
    print("è«‹å®‰è£ requests: pip3 install requests", file=sys.stderr)
    sys.exit(1)

BASE_DIR = Path(__file__).parent.parent
EXTRACTOR_DIR = BASE_DIR / "docs" / "Extractor"
RAW_DIR = EXTRACTOR_DIR / "ingredient_map" / "raw"

# RxNorm API
RXNORM_BASE = "https://rxnav.nlm.nih.gov/REST"
RATE_LIMIT = 0.1  # 10 requests/second

# ç”¢å“ Layer æ¸…å–®
PRODUCT_LAYERS = ["us_dsld", "ca_lnhpd", "kr_hff", "jp_fnfc", "jp_foshu", "tw_hf"]


def extract_ingredients_from_file(filepath: Path) -> list:
    """å¾ç”¢å“ .md æª”æ¡ˆèƒå–æˆåˆ†"""
    ingredients = []

    try:
        content = filepath.read_text(encoding="utf-8")
    except Exception:
        return []

    # è·³é REVIEW_NEEDED æª”æ¡ˆ
    if "[REVIEW_NEEDED]" in content[:500]:
        return []

    # æ‰¾åˆ°æˆåˆ†å€å¡Š
    in_ingredients = False
    for line in content.split("\n"):
        line = line.strip()

        # æˆåˆ†å€å¡Šé–‹å§‹
        if line.startswith("## æˆåˆ†") or line.startswith("## æ©Ÿèƒ½æ€§æˆåˆ†"):
            in_ingredients = True
            continue

        # å…¶ä»–å€å¡Šé–‹å§‹ï¼ŒçµæŸæˆåˆ†æ“·å–
        if line.startswith("## ") and in_ingredients:
            break

        # æ“·å–æˆåˆ†é …ç›®
        if in_ingredients and line.startswith("- "):
            # æ¸…ç†æˆåˆ†åç¨±
            ingredient = line[2:].strip()

            # ç§»é™¤æ‹¬è™Ÿå…§å®¹å’ŒåŠ‘é‡
            ingredient = re.sub(r"\s*[\(ï¼ˆ].*?[\)ï¼‰]", "", ingredient)
            ingredient = re.sub(r"\s*â€”.*$", "", ingredient)
            ingredient = re.sub(r"\s*-\s*\d+.*$", "", ingredient)
            ingredient = re.sub(r"\s+\d+\s*(mg|mcg|iu|g|ml).*$", "", ingredient, flags=re.IGNORECASE)

            ingredient = ingredient.strip()

            if ingredient and len(ingredient) > 1:
                ingredients.append(ingredient)

    return ingredients


def extract_all_ingredients() -> Counter:
    """å¾æ‰€æœ‰ç”¢å“èƒå–æˆåˆ†é »ç‡"""
    print("ğŸ“Š èƒå–æ‰€æœ‰ç”¢å“æˆåˆ†...")

    ingredient_counter = Counter()
    file_count = 0

    for layer in PRODUCT_LAYERS:
        layer_dir = EXTRACTOR_DIR / layer
        if not layer_dir.exists():
            continue

        print(f"  è™•ç† {layer}...")

        # éæ­·æ‰€æœ‰å­ç›®éŒ„ï¼ˆcategoryï¼‰
        for category_dir in layer_dir.iterdir():
            if not category_dir.is_dir() or category_dir.name == "raw":
                continue

            for md_file in category_dir.glob("*.md"):
                ingredients = extract_ingredients_from_file(md_file)
                ingredient_counter.update(ingredients)
                file_count += 1

    print(f"  æƒæ {file_count} å€‹ç”¢å“æª”æ¡ˆ")
    print(f"  ç™¼ç¾ {len(ingredient_counter)} å€‹ç¨ç‰¹æˆåˆ†")

    return ingredient_counter


def query_rxnorm(term: str) -> dict:
    """æŸ¥è©¢ RxNorm API"""
    result = {
        "term": term,
        "rxnorm_id": None,
        "standard_name": None,
        "match_type": None,
        "confidence": "low"
    }

    # ç²¾ç¢ºæŸ¥è©¢
    try:
        url = f"{RXNORM_BASE}/rxcui.json"
        params = {"name": term, "search": 1}
        resp = requests.get(url, params=params, timeout=10)

        if resp.status_code == 200:
            data = resp.json()
            id_group = data.get("idGroup", {})
            rxnorm_ids = id_group.get("rxnormId", [])

            if rxnorm_ids:
                result["rxnorm_id"] = rxnorm_ids[0]
                result["match_type"] = "exact"
                result["confidence"] = "high"

                # å–å¾—æ¨™æº–åç¨±
                prop_url = f"{RXNORM_BASE}/rxcui/{rxnorm_ids[0]}/properties.json"
                prop_resp = requests.get(prop_url, timeout=10)
                if prop_resp.status_code == 200:
                    props = prop_resp.json().get("properties", {})
                    result["standard_name"] = props.get("name", term)

                return result
    except Exception:
        pass

    # æ¨¡ç³ŠæŸ¥è©¢
    try:
        url = f"{RXNORM_BASE}/approximateTerm.json"
        params = {"term": term, "maxEntries": 3}
        resp = requests.get(url, params=params, timeout=10)

        if resp.status_code == 200:
            data = resp.json()
            candidates = data.get("approximateGroup", {}).get("candidate", [])

            if candidates:
                best = candidates[0]
                result["rxnorm_id"] = best.get("rxcui")
                result["standard_name"] = best.get("name", term)
                result["match_type"] = "approximate"

                # æ ¹æ“šåˆ†æ•¸è¨­å®šä¿¡å¿ƒåº¦
                score = int(best.get("score", 0))
                if score >= 90:
                    result["confidence"] = "high"
                elif score >= 70:
                    result["confidence"] = "medium"
                else:
                    result["confidence"] = "low"

                return result
    except Exception:
        pass

    return result


def normalize_ingredients(ingredients: Counter, top_n: int = 500) -> list:
    """æ¨™æº–åŒ–æˆåˆ†"""
    print(f"ğŸ”„ æ¨™æº–åŒ–å‰ {top_n} åæˆåˆ†...")

    # å–å‰ N å
    top_ingredients = ingredients.most_common(top_n)

    results = []

    for i, (ingredient, count) in enumerate(top_ingredients, 1):
        print(f"  [{i}/{len(top_ingredients)}] {ingredient} ({count})")

        rxnorm_result = query_rxnorm(ingredient)
        rxnorm_result["frequency"] = count
        rxnorm_result["queried_at"] = datetime.now().isoformat()

        results.append(rxnorm_result)

        # é€Ÿç‡é™åˆ¶
        time.sleep(RATE_LIMIT)

    return results


def save_results(frequency: Counter, normalized: list):
    """å„²å­˜çµæœ"""
    RAW_DIR.mkdir(parents=True, exist_ok=True)
    today = datetime.now().strftime("%Y-%m-%d")

    # å„²å­˜é »ç‡çµ±è¨ˆ
    freq_file = RAW_DIR / f"ingredient_frequency_{today}.json"
    freq_data = [{"ingredient": ing, "count": cnt} for ing, cnt in frequency.most_common()]
    with open(freq_file, "w", encoding="utf-8") as f:
        json.dump(freq_data, f, ensure_ascii=False, indent=2)
    print(f"  ğŸ“ é »ç‡çµ±è¨ˆ â†’ {freq_file}")

    # å„²å­˜æ¨™æº–åŒ–çµæœ
    if normalized:
        norm_file = RAW_DIR / f"normalized_{today}.jsonl"
        with open(norm_file, "w", encoding="utf-8") as f:
            for item in normalized:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")
        print(f"  ğŸ“ æ¨™æº–åŒ–çµæœ â†’ {norm_file}")

        # çµ±è¨ˆ
        high_conf = sum(1 for x in normalized if x["confidence"] == "high")
        med_conf = sum(1 for x in normalized if x["confidence"] == "medium")
        low_conf = sum(1 for x in normalized if x["confidence"] == "low")
        matched = sum(1 for x in normalized if x["rxnorm_id"])

        print(f"\nğŸ“ˆ æ¨™æº–åŒ–çµ±è¨ˆ:")
        print(f"   åŒ¹é…æˆåŠŸï¼š{matched}/{len(normalized)} ({100*matched/len(normalized):.1f}%)")
        print(f"   é«˜ä¿¡å¿ƒåº¦ï¼š{high_conf}")
        print(f"   ä¸­ä¿¡å¿ƒåº¦ï¼š{med_conf}")
        print(f"   ä½ä¿¡å¿ƒåº¦ï¼š{low_conf}")


def show_stats():
    """é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š"""
    print("ğŸ“Š æˆåˆ†çµ±è¨ˆè³‡è¨Š")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

    # æ‰¾æœ€æ–°çš„é »ç‡æª”æ¡ˆ
    freq_files = list(RAW_DIR.glob("ingredient_frequency_*.json"))
    if not freq_files:
        print("  å°šç„¡çµ±è¨ˆè³‡æ–™ï¼Œè«‹å…ˆåŸ·è¡Œ --extract-all")
        return

    latest = max(freq_files, key=lambda x: x.stat().st_mtime)

    with open(latest, "r", encoding="utf-8") as f:
        data = json.load(f)

    print(f"  æª”æ¡ˆï¼š{latest.name}")
    print(f"  ç¨ç‰¹æˆåˆ†ï¼š{len(data)}")
    print(f"\n  å‰ 20 åæˆåˆ†ï¼š")

    for i, item in enumerate(data[:20], 1):
        print(f"    {i:2}. {item['ingredient']}: {item['count']}")


def main():
    parser = argparse.ArgumentParser(description="æˆåˆ†æ¨™æº–åŒ–æ“·å–")
    parser.add_argument("--extract-all", action="store_true", help="èƒå–æ‰€æœ‰ç”¢å“æˆåˆ†")
    parser.add_argument("--normalize", action="store_true", help="æ¨™æº–åŒ–æˆåˆ†ï¼ˆéœ€å…ˆ --extract-allï¼‰")
    parser.add_argument("--top", type=int, default=500, help="æ¨™æº–åŒ–å‰ N åæˆåˆ†")
    parser.add_argument("--full", action="store_true", help="å®Œæ•´æµç¨‹ï¼ˆèƒå– + æ¨™æº–åŒ–ï¼‰")
    parser.add_argument("--stats", action="store_true", help="é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š")
    args = parser.parse_args()

    if args.stats:
        show_stats()
        return

    if args.full or args.extract_all:
        frequency = extract_all_ingredients()

        if args.full or args.normalize:
            normalized = normalize_ingredients(frequency, args.top)
            save_results(frequency, normalized)
        else:
            save_results(frequency, [])
        return

    if args.normalize:
        # å¾ç¾æœ‰æª”æ¡ˆè¼‰å…¥é »ç‡
        freq_files = list(RAW_DIR.glob("ingredient_frequency_*.json"))
        if not freq_files:
            print("è«‹å…ˆåŸ·è¡Œ --extract-all", file=sys.stderr)
            sys.exit(1)

        latest = max(freq_files, key=lambda x: x.stat().st_mtime)
        with open(latest, "r", encoding="utf-8") as f:
            data = json.load(f)

        frequency = Counter({item["ingredient"]: item["count"] for item in data})
        normalized = normalize_ingredients(frequency, args.top)
        save_results(frequency, normalized)
        return

    parser.print_help()


if __name__ == "__main__":
    main()
