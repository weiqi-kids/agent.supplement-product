#!/usr/bin/env python3
"""PubMed èƒå–è…³æœ¬ â€” å°‡ JSONL è½‰æ›ç‚º .md æª”ä¸¦åˆ†æå…§å®¹"""
import json
import os
import sys
import re
import glob
import argparse
from datetime import datetime, timezone

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
RAW_DIR = os.path.join(BASE_DIR, "docs/Extractor/pubmed/raw")
OUTPUT_DIR = os.path.join(BASE_DIR, "docs/Extractor/pubmed")

# Study Type åˆ¤å®šè¦å‰‡
STUDY_TYPE_RULES = [
    (["Meta-Analysis"], "meta_analysis", 1),
    (["Systematic Review"], "systematic_review", 1),
    (["Randomized Controlled Trial"], "rct", 2),
    (["Clinical Trial"], "clinical_trial", 3),
    (["Observational Study", "Cohort Study"], "observational", 4),
    (["Case Reports"], "case_report", 5),
    (["Review"], "review", 5),
]

# Claim Category é—œéµè©å°æ‡‰
CLAIM_CATEGORY_KEYWORDS = {
    "anti_aging": ["aging", "longevity", "senescence", "telomere", "anti-aging", "lifespan"],
    "cardiovascular": ["cardiovascular", "lipid", "cholesterol", "blood pressure", "heart",
                       "triglyceride", "atherosclerosis", "hypertension"],
    "cognitive": ["cognitive", "memory", "brain", "neurological", "neurodegenerative",
                  "alzheimer", "dementia", "neuroprotective"],
    "immune": ["immune", "inflammation", "inflammatory", "autoimmune", "cytokine",
               "immunomodulat"],
    "metabolic": ["glucose", "diabetes", "metabolism", "obesity", "insulin", "glycemic",
                  "weight loss", "body weight", "metabolic syndrome"],
    "musculoskeletal": ["muscle", "bone", "joint", "osteoporosis", "arthritis", "skeletal",
                        "sarcopenia", "bone density"],
    "sexual": ["sexual", "erectile", "libido", "testosterone", "fertility"],
    "skin": ["skin", "dermatological", "collagen", "wrinkle", "photoaging"],
    "digestive": ["gut", "intestinal", "microbiome", "digestive", "gastrointestinal",
                  "probiotic", "prebiotic"],
    "energy": ["fatigue", "energy", "vitality", "physical performance", "exercise capacity"],
}

# å¸¸è¦‹è£œå……åŠ‘æˆåˆ†
COMMON_INGREDIENTS = [
    "omega-3", "omega-6", "EPA", "DHA", "fish oil", "krill oil",
    "vitamin D", "vitamin C", "vitamin E", "vitamin B12", "vitamin B6",
    "vitamin A", "vitamin K", "folate", "folic acid",
    "calcium", "magnesium", "zinc", "iron", "selenium", "chromium",
    "curcumin", "resveratrol", "quercetin", "CoQ10", "coenzyme Q10",
    "glucosamine", "chondroitin", "collagen", "hyaluronic acid",
    "probiotics", "prebiotics", "fiber", "psyllium",
    "melatonin", "valerian", "ashwagandha", "ginseng", "ginkgo",
    "green tea", "EGCG", "caffeine", "L-theanine",
    "creatine", "BCAA", "whey protein", "casein",
    "astaxanthin", "lutein", "zeaxanthin", "beta-carotene",
    "NAC", "glutathione", "alpha-lipoic acid",
    "berberine", "cinnamon", "bitter melon",
    "saw palmetto", "maca", "tribulus",
    "garlic", "turmeric", "ginger",
    "exosome", "exosomes", "stem cell",
]


def s(val):
    """Safely convert to string, handling None."""
    return str(val).strip() if val is not None else ""


def infer_study_type(publication_types: list, title: str, abstract: str) -> tuple:
    """æ¨æ–·ç ”ç©¶é¡å‹èˆ‡è­‰æ“šç­‰ç´š"""
    combined_text = f"{title} {abstract}".lower()
    pub_types_str = " ".join(publication_types).lower()

    for keywords, study_type, level in STUDY_TYPE_RULES:
        for kw in keywords:
            if kw.lower() in pub_types_str:
                return study_type, level
            # ä¹Ÿæª¢æŸ¥æ¨™é¡Œä¸­çš„ systematic review
            if study_type == "systematic_review" and "systematic review" in combined_text:
                return study_type, level
            if study_type == "meta_analysis" and "meta-analysis" in combined_text:
                return study_type, level

    return "other", 5


def analyze_claim_categories(title: str, abstract: str) -> list:
    """åˆ†ææ‘˜è¦å…§å®¹ï¼Œè­˜åˆ¥å¥åº·åŠŸæ•ˆåˆ†é¡"""
    combined_text = f"{title} {abstract}".lower()
    categories = []

    for category, keywords in CLAIM_CATEGORY_KEYWORDS.items():
        for kw in keywords:
            if kw.lower() in combined_text:
                categories.append(category)
                break

    return categories if categories else ["other"]


def extract_ingredients(title: str, abstract: str) -> list:
    """å¾æ¨™é¡Œå’Œæ‘˜è¦ä¸­æå–æåŠçš„æˆåˆ†"""
    combined_text = f"{title} {abstract}".lower()
    found = []

    for ingredient in COMMON_INGREDIENTS:
        if ingredient.lower() in combined_text:
            found.append(ingredient)

    return found


def check_review_needed(article: dict) -> list:
    """æª¢æŸ¥æ˜¯å¦éœ€è¦æ¨™è¨˜ REVIEW_NEEDED"""
    reasons = []
    if not s(article.get("pmid")):
        reasons.append("PMID ç‚ºç©º")
    if not s(article.get("title")):
        reasons.append("æ¨™é¡Œç‚ºç©º")
    if not s(article.get("abstract")):
        reasons.append("æ‘˜è¦ç‚ºç©º")
    return reasons


def get_existing_source_ids(topic_id: str) -> set:
    """å–å¾—ç‰¹å®šä¸»é¡Œå·²å­˜åœ¨çš„ source_id"""
    ids = set()
    topic_dir = os.path.join(OUTPUT_DIR, topic_id)
    if not os.path.exists(topic_dir):
        return ids

    for f in os.listdir(topic_dir):
        if f.endswith(".md"):
            path = os.path.join(topic_dir, f)
            try:
                with open(path, "r", encoding="utf-8") as fh:
                    for line in fh:
                        if line.startswith("source_id:"):
                            sid = line.split(":", 1)[1].strip().strip('"')
                            ids.add(sid)
                            break
            except:
                pass
    return ids


def find_jsonl_files(topic_id: str = None) -> list:
    """æ‰¾åˆ°è¦è™•ç†çš„ JSONL æª”æ¡ˆ"""
    if topic_id:
        pattern = os.path.join(RAW_DIR, f"{topic_id}-*.jsonl")
    else:
        pattern = os.path.join(RAW_DIR, "*.jsonl")

    files = glob.glob(pattern)
    return sorted(files, key=os.path.getmtime, reverse=True)


def process_file(jsonl_file: str, force: bool = False) -> dict:
    """è™•ç†å–®ä¸€ JSONL æª”æ¡ˆ"""
    stats = {"total": 0, "skipped": 0, "extracted": 0, "review_needed": 0, "errors": 0}
    now = datetime.now(timezone.utc).isoformat()

    # å¾æª”åå–å¾— topic_idï¼ˆæ ¼å¼ï¼š{topic_id}-YYYY-MM.jsonlï¼‰
    basename = os.path.basename(jsonl_file)
    # ç§»é™¤ .jsonl å’Œæ—¥æœŸéƒ¨åˆ† (YYYY-MM)
    name_without_ext = basename.replace(".jsonl", "")
    # æ—¥æœŸæ ¼å¼æ˜¯ YYYY-MMï¼Œæ‰€ä»¥ç§»é™¤æœ€å¾Œå…©å€‹ç”¨ - åˆ†éš”çš„éƒ¨åˆ†
    parts = name_without_ext.rsplit("-", 2)
    if len(parts) >= 3 and parts[-2].isdigit() and parts[-1].isdigit():
        topic_id = "-".join(parts[:-2])
    else:
        topic_id = parts[0]

    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    topic_output_dir = os.path.join(OUTPUT_DIR, topic_id)
    os.makedirs(topic_output_dir, exist_ok=True)

    # å–å¾—å·²å­˜åœ¨çš„ ID
    existing_ids = get_existing_source_ids(topic_id) if not force else set()

    with open(jsonl_file, "r", encoding="utf-8") as f:
        for line_num, raw_line in enumerate(f, 1):
            stats["total"] += 1
            raw_line = raw_line.strip()
            if not raw_line:
                continue

            try:
                article = json.loads(raw_line)
            except json.JSONDecodeError as e:
                print(f"  Line {line_num}: JSON parse error: {e}", file=sys.stderr)
                stats["errors"] += 1
                continue

            pmid = s(article.get("pmid"))
            if not pmid:
                stats["errors"] += 1
                continue

            if pmid in existing_ids:
                stats["skipped"] += 1
                continue

            title = s(article.get("title"))
            journal = s(article.get("journal"))
            pub_date = s(article.get("pub_date"))
            authors = article.get("authors", [])
            abstract = s(article.get("abstract"))
            pub_types = article.get("publication_types", [])

            # æ¨æ–·ç ”ç©¶é¡å‹
            study_type, evidence_level = infer_study_type(pub_types, title, abstract)

            # åˆ†æåŠŸæ•ˆåˆ†é¡
            claim_categories = analyze_claim_categories(title, abstract)

            # æå–æˆåˆ†
            ingredients = extract_ingredients(title, abstract)

            # æª¢æŸ¥ REVIEW_NEEDED
            review_reasons = check_review_needed(article)
            review_prefix = ""
            if review_reasons:
                review_prefix = "[REVIEW_NEEDED]\n\n"
                stats["review_needed"] += 1

            # å®‰å…¨è™•ç†å­—ä¸²
            safe_title = title.replace('"', '\\"')
            source_url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"

            # æ ¼å¼åŒ–ä½œè€…åˆ—è¡¨
            authors_str = ", ".join(authors[:5])
            if len(authors) > 5:
                authors_str += f" et al. (+{len(authors)-5})"

            # æ ¼å¼åŒ– claim_categories ç‚º YAML åˆ—è¡¨
            categories_yaml = json.dumps(claim_categories, ensure_ascii=False)
            ingredients_yaml = json.dumps(ingredients, ensure_ascii=False)

            # å»ºæ§‹ Markdown
            md = f"""{review_prefix}---
source_id: "{pmid}"
source_layer: "pubmed"
source_url: "{source_url}"
market: "global"
title: "{safe_title}"
journal: "{journal}"
pub_date: "{pub_date}"
study_type: "{study_type}"
evidence_level: {evidence_level}
topic: "{topic_id}"
claim_categories: {categories_yaml}
ingredients_mentioned: {ingredients_yaml}
fetched_at: "{now}"
---

# {title}

## åŸºæœ¬è³‡è¨Š
- æœŸåˆŠï¼š{journal}
- ç™¼è¡¨æ—¥æœŸï¼š{pub_date}
- ç ”ç©¶é¡å‹ï¼š{study_type}
- è­‰æ“šç­‰ç´šï¼šLevel {evidence_level}
- ä¸»é¡Œï¼š{topic_id}
- PMIDï¼š{pmid}

## ä½œè€…
{authors_str if authors_str else "ï¼ˆç„¡è³‡æ–™ï¼‰"}

## æ‘˜è¦
{abstract if abstract else "ï¼ˆç„¡è³‡æ–™ï¼‰"}

## åŠŸæ•ˆåˆ†é¡
{", ".join(claim_categories)}

## æåŠæˆåˆ†
{", ".join(ingredients) if ingredients else "ï¼ˆç„¡æ˜ç¢ºæˆåˆ†æåŠï¼‰"}
"""

            # å¯«å…¥æª”æ¡ˆ
            filepath = os.path.join(topic_output_dir, f"{pmid}.md")
            with open(filepath, "w", encoding="utf-8") as out:
                out.write(md)

            existing_ids.add(pmid)
            stats["extracted"] += 1

    return stats


def main():
    parser = argparse.ArgumentParser(description="PubMed èƒå–")
    parser.add_argument("--topic", help="æŒ‡å®šä¸»é¡Œ ID")
    parser.add_argument("--file", help="æŒ‡å®š JSONL æª”æ¡ˆ")
    parser.add_argument("--force", action="store_true", help="å¼·åˆ¶è¦†è“‹å·²å­˜åœ¨æª”æ¡ˆ")
    args = parser.parse_args()

    if args.file:
        files = [args.file] if os.path.exists(args.file) else []
    else:
        files = find_jsonl_files(args.topic)

    if not files:
        print("æœªæ‰¾åˆ° JSONL æª”æ¡ˆ", file=sys.stderr)
        sys.exit(1)

    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("ğŸ“š PubMed èƒå–")
    print(f"   æª”æ¡ˆæ•¸ï¼š{len(files)}")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

    total_stats = {"total": 0, "skipped": 0, "extracted": 0, "review_needed": 0, "errors": 0}

    for jsonl_file in files:
        print(f"\nğŸ“– è™•ç†: {os.path.basename(jsonl_file)}")
        stats = process_file(jsonl_file, force=args.force)

        for k in total_stats:
            total_stats[k] += stats[k]

        print(f"  ç¸½è¡Œæ•¸ï¼š{stats['total']}")
        print(f"  è·³éï¼š{stats['skipped']}")
        print(f"  èƒå–ï¼š{stats['extracted']}")
        print(f"  REVIEW_NEEDEDï¼š{stats['review_needed']}")

    print("\nâ”â”â” pubmed èƒå–å®Œæˆ â”â”â”")
    print(f"  ç¸½è¡Œæ•¸ï¼š{total_stats['total']}")
    print(f"  è·³éï¼ˆå·²å­˜åœ¨ï¼‰ï¼š{total_stats['skipped']}")
    print(f"  æ–°èƒå–ï¼š{total_stats['extracted']}")
    print(f"  REVIEW_NEEDEDï¼š{total_stats['review_needed']}")
    print(f"  éŒ¯èª¤ï¼š{total_stats['errors']}")


if __name__ == "__main__":
    main()
