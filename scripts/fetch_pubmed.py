#!/usr/bin/env python3
"""PubMed æ–‡ç»æ“·å–è…³æœ¬ â€” å¾ NCBI E-utilities API æŸ¥è©¢ä¸»é¡Œç›¸é—œæ–‡ç»"""
import json
import os
import sys
import argparse
import time
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from urllib.request import urlopen, Request
from urllib.parse import urlencode
from urllib.error import HTTPError, URLError

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
TOPICS_DIR = os.path.join(BASE_DIR, "core/Narrator/Modes/topic_tracking/topics")
RAW_DIR = os.path.join(BASE_DIR, "docs/Extractor/pubmed/raw")

# NCBI API endpoints
ESEARCH_URL = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
EFETCH_URL = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
API_KEY = os.environ.get("NCBI_API_KEY", "")
EMAIL = os.environ.get("NCBI_EMAIL", "")

# é€Ÿç‡é™åˆ¶ï¼ˆç„¡ API Key: 3/s, æœ‰ API Key: 10/sï¼‰
RATE_LIMIT = 0.35 if API_KEY else 0.5


def load_topic_config(topic_id: str) -> dict:
    """è¼‰å…¥ä¸»é¡Œè¨­å®šæª”"""
    import yaml
    topic_file = os.path.join(TOPICS_DIR, f"{topic_id}.yaml")
    if not os.path.exists(topic_file):
        print(f"ä¸»é¡Œè¨­å®šæª”ä¸å­˜åœ¨: {topic_file}", file=sys.stderr)
        sys.exit(1)

    with open(topic_file, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def build_pubmed_query(topic_config: dict) -> str:
    """å¾ä¸»é¡Œè¨­å®šå»ºæ§‹ PubMed æŸ¥è©¢"""
    # æª¢æŸ¥æ˜¯å¦æœ‰è‡ªè¨‚ pubmed æŸ¥è©¢
    if "pubmed" in topic_config and "query" in topic_config["pubmed"]:
        return topic_config["pubmed"]["query"]

    # å¾ keywords å»ºæ§‹æŸ¥è©¢
    keywords = topic_config.get("keywords", {})
    exact_terms = keywords.get("exact", [])
    fuzzy_terms = keywords.get("fuzzy", [])

    # çµ„åˆç²¾ç¢ºé—œéµè©
    all_terms = exact_terms + fuzzy_terms
    if not all_terms:
        print("ä¸»é¡Œç„¡é—œéµè©å®šç¾©", file=sys.stderr)
        sys.exit(1)

    # å»ºæ§‹æŸ¥è©¢ï¼šé—œéµè© OR çµ„åˆ + è£œå……é¡å‹ç¯©é¸
    term_parts = [f'"{term}"[Title/Abstract]' for term in all_terms[:10]]  # é™åˆ¶æœ€å¤š 10 å€‹
    base_query = " OR ".join(term_parts)

    # åŠ ä¸Šè£œå……å“ç›¸é—œç¯©é¸
    supplement_filter = '(supplement*[Title/Abstract] OR nutraceutical*[Title/Abstract] OR "dietary supplement"[Title/Abstract])'

    return f"({base_query}) AND {supplement_filter}"


def esearch(query: str, max_results: int = 500, date_range_years: int = 5) -> list:
    """åŸ·è¡Œ ESearch å–å¾— PMID åˆ—è¡¨"""
    # è¨ˆç®—æ—¥æœŸç¯„åœ
    end_date = datetime.now()
    start_date = end_date - timedelta(days=date_range_years * 365)

    params = {
        "db": "pubmed",
        "term": query,
        "retmax": max_results,
        "retmode": "json",
        "mindate": start_date.strftime("%Y/%m/%d"),
        "maxdate": end_date.strftime("%Y/%m/%d"),
        "datetype": "pdat",
        "usehistory": "n"
    }

    if API_KEY:
        params["api_key"] = API_KEY
    if EMAIL:
        params["email"] = EMAIL

    url = f"{ESEARCH_URL}?{urlencode(params)}"

    try:
        req = Request(url, headers={"User-Agent": "SupplementProductAgent/1.0"})
        with urlopen(req, timeout=30) as response:
            data = json.load(response)
    except (HTTPError, URLError, json.JSONDecodeError) as e:
        print(f"ESearch å¤±æ•—: {e}", file=sys.stderr)
        return []

    result = data.get("esearchresult", {})
    pmids = result.get("idlist", [])
    count = result.get("count", "0")

    print(f"  æ‰¾åˆ° {count} ç¯‡æ–‡ç»ï¼Œå–å¾— {len(pmids)} å€‹ PMID")
    return pmids


def efetch_batch(pmids: list, batch_size: int = 100) -> list:
    """æ‰¹æ¬¡åŸ·è¡Œ EFetch å–å¾—æ–‡ç»è©³ç´°è³‡æ–™"""
    all_articles = []

    for i in range(0, len(pmids), batch_size):
        batch = pmids[i:i + batch_size]
        print(f"  å–å¾—æ–‡ç» {i+1}-{min(i+batch_size, len(pmids))}...")

        params = {
            "db": "pubmed",
            "id": ",".join(batch),
            "rettype": "xml",
            "retmode": "xml"
        }

        if API_KEY:
            params["api_key"] = API_KEY
        if EMAIL:
            params["email"] = EMAIL

        url = f"{EFETCH_URL}?{urlencode(params)}"

        try:
            req = Request(url, headers={"User-Agent": "SupplementProductAgent/1.0"})
            with urlopen(req, timeout=60) as response:
                xml_content = response.read()

            articles = parse_pubmed_xml(xml_content)
            all_articles.extend(articles)

        except (HTTPError, URLError, ET.ParseError) as e:
            print(f"  EFetch æ‰¹æ¬¡å¤±æ•—: {e}", file=sys.stderr)

        # é€Ÿç‡é™åˆ¶
        time.sleep(RATE_LIMIT)

    return all_articles


def parse_pubmed_xml(xml_content: bytes) -> list:
    """è§£æ PubMed XML å›æ‡‰"""
    articles = []

    try:
        root = ET.fromstring(xml_content)
    except ET.ParseError as e:
        print(f"  XML è§£æå¤±æ•—: {e}", file=sys.stderr)
        return []

    for article_elem in root.findall(".//PubmedArticle"):
        article = {}

        # PMID
        pmid_elem = article_elem.find(".//PMID")
        article["pmid"] = pmid_elem.text if pmid_elem is not None else ""

        # æ¨™é¡Œ
        title_elem = article_elem.find(".//ArticleTitle")
        article["title"] = title_elem.text if title_elem is not None else ""

        # æœŸåˆŠ
        journal_elem = article_elem.find(".//Journal/Title")
        article["journal"] = journal_elem.text if journal_elem is not None else ""

        # ç™¼è¡¨æ—¥æœŸ
        pub_date = article_elem.find(".//PubDate")
        if pub_date is not None:
            year = pub_date.findtext("Year", "")
            month = pub_date.findtext("Month", "01")
            day = pub_date.findtext("Day", "01")
            # æœˆä»½å¯èƒ½æ˜¯æ–‡å­—
            month_map = {"Jan": "01", "Feb": "02", "Mar": "03", "Apr": "04",
                        "May": "05", "Jun": "06", "Jul": "07", "Aug": "08",
                        "Sep": "09", "Oct": "10", "Nov": "11", "Dec": "12"}
            month = month_map.get(month, month.zfill(2) if month.isdigit() else "01")
            article["pub_date"] = f"{year}-{month}-{day.zfill(2)}" if year else ""
        else:
            article["pub_date"] = ""

        # ä½œè€…
        authors = []
        for author in article_elem.findall(".//Author"):
            lastname = author.findtext("LastName", "")
            forename = author.findtext("ForeName", "")
            if lastname:
                authors.append(f"{lastname} {forename}".strip())
        article["authors"] = authors

        # æ‘˜è¦
        abstract_texts = []
        for abstract_text in article_elem.findall(".//AbstractText"):
            label = abstract_text.get("Label", "")
            text = abstract_text.text or ""
            if label:
                abstract_texts.append(f"**{label}**: {text}")
            else:
                abstract_texts.append(text)
        article["abstract"] = "\n\n".join(abstract_texts)

        # å‡ºç‰ˆé¡å‹
        pub_types = []
        for pub_type in article_elem.findall(".//PublicationType"):
            if pub_type.text:
                pub_types.append(pub_type.text)
        article["publication_types"] = pub_types

        # MeSH è¡“èª
        mesh_terms = []
        for mesh in article_elem.findall(".//MeshHeading/DescriptorName"):
            if mesh.text:
                mesh_terms.append(mesh.text)
        article["mesh_terms"] = mesh_terms

        # é—œéµè©
        keywords = []
        for keyword in article_elem.findall(".//Keyword"):
            if keyword.text:
                keywords.append(keyword.text)
        article["keywords"] = keywords

        articles.append(article)

    return articles


def save_to_jsonl(articles: list, topic_id: str) -> str:
    """å°‡æ–‡ç»è³‡æ–™å„²å­˜ç‚º JSONL"""
    os.makedirs(RAW_DIR, exist_ok=True)

    today = datetime.now().strftime("%Y-%m")
    output_file = os.path.join(RAW_DIR, f"{topic_id}-{today}.jsonl")

    with open(output_file, "w", encoding="utf-8") as f:
        for article in articles:
            article["topic"] = topic_id
            article["fetched_at"] = datetime.now().isoformat()
            f.write(json.dumps(article, ensure_ascii=False) + "\n")

    return output_file


def list_topics() -> list:
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨ä¸»é¡Œ"""
    import yaml
    topics = []
    if not os.path.exists(TOPICS_DIR):
        return topics

    for f in os.listdir(TOPICS_DIR):
        if f.endswith(".yaml"):
            topic_id = f.replace(".yaml", "")
            topics.append(topic_id)

    return topics


def main():
    parser = argparse.ArgumentParser(description="PubMed æ–‡ç»æ“·å–")
    parser.add_argument("--topic", help="æŒ‡å®šä¸»é¡Œ ID")
    parser.add_argument("--all", action="store_true", help="æ“·å–æ‰€æœ‰ä¸»é¡Œ")
    parser.add_argument("--limit", type=int, default=500, help="æ¯ä¸»é¡Œæœ€å¤§çµæœæ•¸")
    parser.add_argument("--years", type=int, default=5, help="æŸ¥è©¢å¹´æ•¸ç¯„åœ")
    parser.add_argument("--list", action="store_true", help="åˆ—å‡ºå¯ç”¨ä¸»é¡Œ")
    args = parser.parse_args()

    if args.list:
        topics = list_topics()
        print("å¯ç”¨ä¸»é¡Œ:")
        for t in topics:
            print(f"  - {t}")
        return

    if args.all:
        topics = list_topics()
    elif args.topic:
        topics = [args.topic]
    else:
        print("è«‹æŒ‡å®š --topic æˆ– --all", file=sys.stderr)
        sys.exit(1)

    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("ğŸ“š PubMed æ–‡ç»æ“·å–")
    print(f"   ä¸»é¡Œæ•¸ï¼š{len(topics)}")
    print(f"   æ¯ä¸»é¡Œé™åˆ¶ï¼š{args.limit} ç¯‡")
    print(f"   å¹´æ•¸ç¯„åœï¼š{args.years} å¹´")
    if API_KEY:
        print("   API Keyï¼šå·²è¨­å®š")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

    total_articles = 0

    for topic_id in topics:
        print(f"\nğŸ“– è™•ç†ä¸»é¡Œ: {topic_id}")

        try:
            config = load_topic_config(topic_id)
        except Exception as e:
            print(f"  è¼‰å…¥è¨­å®šå¤±æ•—: {e}", file=sys.stderr)
            continue

        query = build_pubmed_query(config)
        print(f"  æŸ¥è©¢: {query[:80]}...")

        # ESearch
        pmids = esearch(query, max_results=args.limit, date_range_years=args.years)
        if not pmids:
            print("  ç„¡çµæœ")
            continue

        # EFetch
        articles = efetch_batch(pmids)
        if not articles:
            print("  å–å¾—æ–‡ç»å¤±æ•—")
            continue

        # å„²å­˜
        output_file = save_to_jsonl(articles, topic_id)
        print(f"  âœ… å„²å­˜ {len(articles)} ç¯‡ â†’ {output_file}")
        total_articles += len(articles)

    print("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print(f"âœ… Fetch completed: pubmed")
    print(f"   ç¸½æ–‡ç»æ•¸ï¼š{total_articles}")


if __name__ == "__main__":
    main()
