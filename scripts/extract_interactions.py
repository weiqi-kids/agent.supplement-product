#!/usr/bin/env python3
"""äº¤äº’ä½œç”¨èƒå–è…³æœ¬ â€” å°‡ JSONL è½‰æ›ç‚º .md æª”ä¸¦åˆ†æå…§å®¹"""
import json
import os
import re
import sys
import argparse
from datetime import datetime
from pathlib import Path

BASE_DIR = Path(__file__).parent.parent

# Study Type åˆ¤å®šè¦å‰‡
STUDY_TYPE_RULES = [
    (["Meta-Analysis"], "meta_analysis", 1),
    (["Systematic Review"], "systematic_review", 1),
    (["Randomized Controlled Trial"], "rct", 2),
    (["Clinical Trial"], "clinical_trial", 3),
    (["Observational Study", "Cohort Study"], "observational", 4),
    (["Case Reports"], "case_report", 5),
    (["Review"], "review", 5),
]

# åš´é‡ç¨‹åº¦é—œéµè©
SEVERITY_KEYWORDS = {
    "major": ["contraindicated", "life-threatening", "serious adverse", "fatal",
              "avoid concomitant", "significantly increase", "bleeding risk",
              "hospitalization", "death", "hemorrhage"],
    "moderate": ["monitor", "caution", "dose adjustment", "clinical significance",
                 "reduce dose", "increased effect", "decreased effect",
                 "should be aware", "consider alternative"],
    "minor": ["minimal", "unlikely", "no significant", "limited clinical",
              "unlikely to be clinically significant"],
}

# è—¥ç‰©é¡åˆ¥é—œéµè©
DRUG_CLASS_KEYWORDS = {
    "anticoagulant": ["warfarin", "heparin", "anticoagulant", "blood thinner",
                      "coumadin", "enoxaparin", "rivaroxaban", "apixaban"],
    "antiplatelet": ["aspirin", "clopidogrel", "plavix", "antiplatelet", "ticagrelor"],
    "antihypertensive": ["ace inhibitor", "arb", "beta blocker", "calcium channel",
                         "antihypertensive", "lisinopril", "amlodipine", "losartan"],
    "antidiabetic": ["metformin", "insulin", "sulfonylurea", "antidiabetic",
                     "glipizide", "glimepiride", "diabetes"],
    "statin": ["statin", "atorvastatin", "simvastatin", "rosuvastatin", "pravastatin"],
    "antidepressant": ["ssri", "snri", "maoi", "antidepressant", "sertraline",
                       "fluoxetine", "escitalopram", "venlafaxine"],
    "immunosuppressant": ["cyclosporine", "tacrolimus", "immunosuppressant",
                          "mycophenolate", "sirolimus"],
    "thyroid": ["levothyroxine", "synthroid", "thyroid", "liothyronine"],
}

# è£œå……åŠ‘é¡åˆ¥é—œéµè©
SUPPLEMENT_CATEGORY_KEYWORDS = {
    "omega_fatty_acid": ["omega-3", "omega 3", "fish oil", "epa", "dha", "krill"],
    "botanical": ["herb", "botanical", "ginkgo", "ginseng", "curcumin", "turmeric",
                  "garlic", "st john", "hypericum", "echinacea", "valerian",
                  "saw palmetto", "green tea"],
    "vitamin": ["vitamin", "ascorbic", "tocopherol", "retinol"],
    "mineral": ["calcium", "magnesium", "zinc", "iron", "selenium"],
    "amino_acid": ["amino acid", "arginine", "glutamine", "carnitine"],
    "enzyme": ["enzyme", "coq10", "nattokinase", "bromelain"],
    "probiotic": ["probiotic", "lactobacillus", "bifidobacterium"],
}

# é£Ÿç‰©é¡åˆ¥é—œéµè©
FOOD_CATEGORY_KEYWORDS = {
    "citrus": ["grapefruit", "citrus", "seville orange", "pomelo"],
    "dairy": ["dairy", "milk", "cheese", "yogurt", "calcium-fortified"],
    "leafy_greens": ["leafy green", "spinach", "kale", "broccoli", "vitamin k food"],
    "high_fat": ["high fat", "fatty meal", "high-fat"],
    "caffeine": ["caffeine", "coffee", "tea", "energy drink"],
    "alcohol": ["alcohol", "ethanol", "wine", "beer"],
    "fermented": ["fermented", "natto", "kimchi", "sauerkraut"],
    "fiber": ["fiber", "whole grain", "bran"],
}


def slugify(text: str) -> str:
    """è½‰æ›ç‚º URL-safe çš„ slug"""
    text = text.lower()
    text = re.sub(r"[^\w\s-]", "", text)
    text = re.sub(r"[-\s]+", "-", text)
    return text.strip("-")[:80]  # é™åˆ¶é•·åº¦


def infer_study_type(publication_types: list, title: str, abstract: str) -> tuple:
    """æ¨æ–·ç ”ç©¶é¡å‹èˆ‡è­‰æ“šç­‰ç´š"""
    combined_text = f"{title} {abstract}".lower()
    pub_types_str = " ".join(publication_types).lower()

    for keywords, study_type, level in STUDY_TYPE_RULES:
        for kw in keywords:
            if kw.lower() in pub_types_str:
                return study_type, level
            if kw.lower() in combined_text:
                return study_type, level

    return "other", 5


def infer_severity(title: str, abstract: str) -> str:
    """æ¨æ–·äº¤äº’åš´é‡ç¨‹åº¦"""
    combined_text = f"{title} {abstract}".lower()

    for severity, keywords in SEVERITY_KEYWORDS.items():
        for kw in keywords:
            if kw in combined_text:
                return severity

    return "unknown"


def categorize_by_keywords(text: str, keyword_map: dict) -> str:
    """æ ¹æ“šé—œéµè©åˆ†é¡"""
    text_lower = text.lower()

    for category, keywords in keyword_map.items():
        for kw in keywords:
            if kw in text_lower:
                return category

    return "other"


def check_review_needed(article: dict) -> list:
    """æª¢æŸ¥æ˜¯å¦éœ€è¦æ¨™è¨˜ REVIEW_NEEDED"""
    reasons = []

    if not article.get("pmid"):
        reasons.append("PMID ç‚ºç©º")
    if not article.get("title"):
        reasons.append("æ¨™é¡Œç‚ºç©º")
    if not article.get("abstract"):
        reasons.append("æ‘˜è¦ç‚ºç©º")

    return reasons


def generate_markdown(article: dict, interaction_type: str, category: str) -> str:
    """ç”Ÿæˆ Markdown å…§å®¹"""
    pmid = article.get("pmid", "")
    title = article.get("title", "")
    abstract = article.get("abstract", "")
    journal = article.get("journal", "")
    pub_date = article.get("pub_date", "")
    authors = article.get("authors", [])
    publication_types = article.get("publication_types", [])

    # æ¨æ–·ç ”ç©¶é¡å‹å’Œè­‰æ“šç­‰ç´š
    study_type, evidence_level = infer_study_type(publication_types, title, abstract)

    # æ¨æ–·åš´é‡ç¨‹åº¦
    severity = infer_severity(title, abstract)

    review_reasons = check_review_needed(article)
    if severity == "unknown" and not review_reasons:
        review_reasons.append("åš´é‡ç¨‹åº¦ç„¡æ³•åˆ¤å®š")

    review_marker = "[REVIEW_NEEDED]\n\n" if review_reasons else ""

    # æ ¼å¼åŒ–ä½œè€…
    authors_str = ", ".join(authors[:5])
    if len(authors) > 5:
        authors_str += f" et al. ({len(authors)} authors)"

    md = f"""{review_marker}---
source_id: "{pmid}"
source_layer: "{interaction_type}"
source_url: "https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
interaction_type: "{interaction_type.upper()}"
category: "{category}"
title: "{title.replace('"', "'")}"
journal: "{journal}"
pub_date: "{pub_date}"
study_type: "{study_type}"
evidence_level: {evidence_level}
severity: "{severity}"
fetched_at: "{article.get('fetched_at', datetime.now().isoformat())}"
---

# {title}

## åŸºæœ¬è³‡è¨Š
- äº¤äº’é¡å‹ï¼š{interaction_type.upper()}
- é¡åˆ¥ï¼š{category}
- PMIDï¼š{pmid}
- æœŸåˆŠï¼š{journal}
- ç™¼è¡¨æ—¥æœŸï¼š{pub_date}
- ç ”ç©¶é¡å‹ï¼š{study_type}
- è­‰æ“šç­‰ç´šï¼šLevel {evidence_level}
- åš´é‡ç¨‹åº¦ï¼š{severity.capitalize()}

## ä½œè€…
{authors_str}

## æ‘˜è¦
{abstract}

---

âš ï¸ **å…è²¬è²æ˜**ï¼šæœ¬è³‡è¨Šåƒ…ä¾›æ•™è‚²å’Œç ”ç©¶ç›®çš„ï¼Œä¸æ§‹æˆé†«ç™‚å»ºè­°ã€‚
ä»»ä½•ç”¨è—¥æˆ–è£œå……åŠ‘è®Šæ›´æ‡‰è«®è©¢å°ˆæ¥­é†«ç™‚äººå“¡ã€‚
"""

    return md


def get_existing_source_ids(layer_dir: Path) -> set:
    """å–å¾—å·²å­˜åœ¨çš„ source_id"""
    existing = set()

    for category_dir in layer_dir.iterdir():
        if not category_dir.is_dir() or category_dir.name == "raw":
            continue

        for md_file in category_dir.glob("*.md"):
            try:
                content = md_file.read_text(encoding="utf-8")
                for line in content.split("\n"):
                    if line.startswith("source_id:"):
                        sid = line.split(":", 1)[1].strip().strip('"')
                        existing.add(sid)
                        break
            except Exception:
                pass

    return existing


def process_jsonl_file(jsonl_path: Path, interaction_type: str, force: bool = False) -> dict:
    """è™•ç†å–®ä¸€ JSONL æª”æ¡ˆ"""
    stats = {"processed": 0, "skipped": 0, "review_needed": 0, "new": 0}

    layer_dir = BASE_DIR / "docs" / "Extractor" / interaction_type
    existing = set() if force else get_existing_source_ids(layer_dir)

    # æ ¹æ“šäº¤äº’é¡å‹é¸æ“‡åˆ†é¡æ–¹å¼
    if interaction_type == "dhi":
        category_keywords = SUPPLEMENT_CATEGORY_KEYWORDS
    elif interaction_type == "dfi":
        category_keywords = FOOD_CATEGORY_KEYWORDS
    else:  # ddi
        category_keywords = DRUG_CLASS_KEYWORDS

    with open(jsonl_path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue

            try:
                article = json.loads(line)
            except json.JSONDecodeError:
                continue

            pmid = article.get("pmid", "")
            if not pmid:
                stats["skipped"] += 1
                continue

            # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if pmid in existing and not force:
                stats["skipped"] += 1
                continue

            # åˆ†é¡
            title = article.get("title", "")
            abstract = article.get("abstract", "")
            combined_text = f"{title} {abstract}"

            file_category = article.get("category", "general")
            content_category = categorize_by_keywords(combined_text, category_keywords)

            # ä½¿ç”¨æª”æ¡ˆé¡åˆ¥æˆ–å…§å®¹åˆ†é¡
            category = content_category if content_category != "other" else file_category

            # å»ºç«‹ç›®éŒ„
            category_dir = layer_dir / category
            category_dir.mkdir(parents=True, exist_ok=True)

            # ç”Ÿæˆ Markdown
            md_content = generate_markdown(article, interaction_type, category)

            # å¯«å…¥æª”æ¡ˆ
            output_path = category_dir / f"{pmid}.md"
            output_path.write_text(md_content, encoding="utf-8")

            stats["processed"] += 1
            stats["new"] += 1

            if check_review_needed(article):
                stats["review_needed"] += 1

    return stats


def main():
    parser = argparse.ArgumentParser(description="äº¤äº’ä½œç”¨èƒå–")
    parser.add_argument("--type", required=True, choices=["ddi", "dfi", "dhi"],
                       help="äº¤äº’é¡å‹")
    parser.add_argument("jsonl_file", nargs="?", help="æŒ‡å®š JSONL æª”æ¡ˆ")
    parser.add_argument("--all", action="store_true", help="è™•ç†æ‰€æœ‰ JSONL æª”æ¡ˆ")
    parser.add_argument("--force", action="store_true", help="å¼·åˆ¶è¦†è“‹å·²å­˜åœ¨çš„æª”æ¡ˆ")
    args = parser.parse_args()

    interaction_type = args.type
    raw_dir = BASE_DIR / "docs" / "Extractor" / interaction_type / "raw"

    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print(f"ğŸ’Š {interaction_type.upper()} èƒå–")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

    # æ‰¾åˆ°è¦è™•ç†çš„æª”æ¡ˆ
    if args.jsonl_file:
        jsonl_files = [Path(args.jsonl_file)]
    elif args.all:
        jsonl_files = list(raw_dir.glob("*.jsonl"))
    else:
        # æ‰¾æœ€æ–°çš„
        jsonl_files = sorted(raw_dir.glob("*.jsonl"),
                            key=lambda x: x.stat().st_mtime,
                            reverse=True)

    if not jsonl_files:
        print(f"  æ‰¾ä¸åˆ° JSONL æª”æ¡ˆæ–¼ {raw_dir}", file=sys.stderr)
        print("  è«‹å…ˆåŸ·è¡Œ fetch.sh", file=sys.stderr)
        sys.exit(1)

    total_stats = {"processed": 0, "skipped": 0, "review_needed": 0, "new": 0}

    for jsonl_path in jsonl_files:
        print(f"\n  è™•ç†: {jsonl_path.name}")
        stats = process_jsonl_file(jsonl_path, interaction_type, args.force)

        for k, v in stats.items():
            total_stats[k] += v

        print(f"    âœ… è™•ç†: {stats['processed']}, è·³é: {stats['skipped']}, REVIEW: {stats['review_needed']}")

    print("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print(f"âœ… èƒå–å®Œæˆ: {interaction_type.upper()}")
    print(f"   è™•ç†: {total_stats['processed']}")
    print(f"   è·³é: {total_stats['skipped']}")
    print(f"   REVIEW_NEEDED: {total_stats['review_needed']}")


if __name__ == "__main__":
    main()
